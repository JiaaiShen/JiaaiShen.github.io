---
layout: post
title: Blog Post 4
---

In this post, I'll write a simple version of the *spectral clustering* algorithm for clustering data points.

Let's begin by importing all the libraries that we'll need in order to write the algorithm in this tutorial.


```python
import numpy as np
from sklearn import datasets
from sklearn.metrics import pairwise_distances
from scipy.optimize import minimize
from matplotlib import pyplot as plt
```

## Introduction

Let's start by looking at a data set where we need spectral clustering to make out two crescents in the data.


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```


    
![two-crescents.png](/images/two-crescents.png)
    


The Euclidean coordinates of the data points are contained in `X`, while the labels of each point are contained in `y`. 

## Part A

In the next cell, we'll use a parameter `epsilon` and create the *similarity matrix* $$\mathbf{A}$$. $$\mathbf{A}$$ is a 2d `np.ndarray` with shape `(n, n)`. Recall that `n` is the number of data points. 

The entries in $$\mathbf{A}$$ should have the following values:
- `A[i,j]` should be equal to `1` if `X[i]` (the coordinates of data point `i`) is within `epsilon` distance of `X[j]` (the coordinates of data point `j`). Otherwise, `A[i,j]` should be equal to `0`.
- `A[i,i]` should all be equal to `0`.


```python
epsilon = 0.4

# A[i,j] = 1 if the distance between data points i and j is less than epsilon
# A[i,j] = 0 if the distance between data points i and j is not within epsilon
A = 1 * (pairwise_distances(X) < epsilon)

# use the np.fill_diagonal() function to set A[i,i] = 0
np.fill_diagonal(A, 0)

A
```




    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 1, 0],
           ...,
           [0, 0, 0, ..., 0, 1, 1],
           [0, 0, 1, ..., 1, 0, 1],
           [0, 0, 0, ..., 1, 1, 0]])



## Part B

We assume that every data point is in either $$C_0$$ or $$C_1$$ where $$C_0$$ and $$C_1$$ are two clusters of the data points. Recall that `y` contains the label of each data point. Data point `i` (i.e. row $$i$$ of $$\mathbf{A}$$) is in cluster $$C_0$$ if `y[i] = 0`. If `y[i] = 1`, then data point `i` is in cluster $$C_1$$.

The *binary norm cut objective* of $$\mathbf{A}$$ is the function

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;$$

where

- $$\mathbf{cut}(C_0, C_1)$$ is the *cut* of the clusters $$C_0$$ and $$C_1$$. This term is the number of nonzero entries in $$\mathbf{A}$$ for each pair of data points in different clusters. 
- $$\mathbf{vol}(C_0)$$ is the *volume* of cluster $$C_0$$. This term is the number of nonzero entries in $$\mathbf{A}$$ for data points in cluster $$C_0$$. Similarly, $$\mathbf{vol}(C_1)$$ is the *volume* of cluster $$C_1$$. This term is the number of nonzero entries in $$\mathbf{A}$$ for data points in cluster $$C_1$$.

Let's look at the two terms separately.

### The Cut Term

First, we'll write a function called `cut(A,y)` to compute the cut term by summing up `A[i,j]` for each pair of data points `i` and `j` in different clusters.


```python
def cut(A,y):
    # initialize the cut term
    cut_term = 0
    
    # loop over the data points
    for i in range(n):
        # check whether data point i is in cluster C0
        if y[i] == 0:
            # loop over the data points
            for j in range(n):
                # check whether data point j is in cluster C1
                if y[j] == 1:
                    # add A[i,j] to the cut term if data points i and j 
                    # are in different clusters
                    cut_term = cut_term + A[i,j]
    
    return(cut_term)
```

We want to compare the cut term for the vector containing the true labels and the cut term for a random vector of the same length but containing random labels equal to `0` or `1`. We'll use the `np.random.randint()` function to generate the random vector of length `n`.


```python
# compute the cut term for y
true_cut = cut(A,y)

# generate a random vector with each entry equal to 0 or 1 
r = np.random.randint(2, size = n)

# compute the cut term for the random vector
random_cut = cut(A,r)

# compare the cut terms
true_cut, random_cut
```




    (13, 1092)



As we can see, the cut term for the true labels `y` is *much* smaller than the cut term for the random labels.

### The Volume Term

We'll now 

- write a function called `vols(A,y)` to compute the volume of cluster $$C_0$$ by summing up `A[i,j]` for each data point `i` in cluster $$C_0$$ and compute the volume of cluster $$C_1$$ by summing up `A[i,j]` for each data point `i` in cluster $$C_1$$.

- write a function called `normcut(A,y)` to compute the binary norm cut objective of `A` by using the `cut()` function and the `vols()` function.


```python
def vols(A,y):
    
    # sum up the entries in A for data points in cluster C0
    # to compute the volume of cluster C0
    v0 = np.sum(A[y == 0,:])
    
    # sum up the entries in A for data points in cluster C1
    # to compute the volume of cluster C1
    v1 = np.sum(A[y == 1,:])
    
    return(v0, v1)

def normcut(A,y):
    
    # use the cut() function to compute the cut term
    c = cut(A,y)
    
    # use the vols() function to compute the volumes
    v0 = vols(A,y)[0]
    v1 = vols(A,y)[1]
    
    return(c * (1 / v0 + 1 / v1))
```

We want to compare the norm cut objective for the true labels `y` and the norm cut objective for the random labels we generated above. 


```python
# compare the norm cut objectives
normcut(A,y), normcut(A,r)
```




    (0.011518412331615225, 0.9768409242712723)



As we can see, the norm cut objective for the true labels `y` is *much* smaller than the norm cut objective for the random labels.

## Part C

Next, we'll write a function called `transform(A,y)` to compute a new vector $$\mathbf{z} \in \mathbb{R}^n$$ with 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
.
$$

Notice that 

- if data point `i` is in cluster $$C_0$$, then `y[i] = 0` and `z[i] > 0`.
- if data point `i` is in cluster $$C_1$$, then `y[i] = 1` and `z[i] < 0`.


```python
def transform(A,y):
    
    # use the vols() function to compute the volumes
    v0 = vols(A,y)[0]
    v1 = vols(A,y)[1]
    
    # initialize the vector
    z = np.ndarray(n)
    
    # compute the vector using the formula
    z[y == 0] = 1 / v0
    z[y == 1] = - 1 / v1

    return(z)
```

Let's check the following equation:

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;$$

where $$\mathbf{D}$$ is a diagonal matrix with nonzero entries $$d_{ii}$$ equal to the $$i$$th row-sum of $$\mathbf{A}$$.


```python
# use the np.diag() function to construct the diagonal matrix D
D = np.diag(A.sum(axis = 1))

# use the transform() function to compute the vector z
z = transform(A,y)

# compute the right side of the equation
RHS = (z@(D - A)@z) / (z@D@z)

# use the normcut() function to compute the norm cut objective of A
N = normcut(A,y)

# use the np.isclose() function to check if N is "close" to RHS
np.isclose(N,RHS)
```




    True



This shows that the left side of the equation above is "close" to the right side.

We also want to check the identity $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$ where $$\mathbb{1}$$ is the vector of `n` ones. We'll use the `np.ones()` function to construct this vector.


```python
# use the np.ones() function to construct the vector of n ones
e = np.ones(n)
# compute the left side of the identity equation
identity = z@D@e
# use the np.isclose() function to check if identity is "close" to 0
np.isclose(identity,0)
```




    True



This shows that the left side of the identity equation is "close" to the right side.

## Part D

We'll write a function called `orth_obj(z)` to replace $$\mathbf{z}$$ with the orthogonal complement of $$\mathbf{z}$$ relative to $$\mathbf{D}\mathbb{1}$$ in the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. In order to compute the orthogonal complement, we'll write a function called `orth(u, v)`.


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

# use the np.ones() function to construct the vector of n ones
e = np.ones(n) 

# compute the product of D and e
d = D @ e

def orth_obj(z):
    # compute the orthogonal complement of z
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

We'll use the `minimize` function from `scipy.optimize` to minimize the `orth_obj()` function with respect to `z` and name the minimizing vector `z_min`. The minimizing vector is the `x` attribute of the `minimize` object.


```python
# minimize the orth_obj() function with a random initial guess
z_min = minimize(orth_obj, np.random.rand(n)).x
```

## Part E

Let's plot the data, using one color for data points such that `z_min[i] < 0` and another color for data points such that `z_min[i] >= 0`.


```python
# plot the data
plt.scatter(X[:,0], X[:,1], c = z_min >= 0)
```


    
![part-e.png](/images/part-e.png)
    


It looks like we come close to correctly cluster the data.

## Part F

The minimizing $$\mathbf{z}$$ must be the eigenvector corresponding to the *second*-smallest eigenvalue of the matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, which is called the (normalized) *Laplacian* matrix of the similarity matrix $$\mathbf{A}$$. So, we'll find the eigenvector with the second-smallest eigenvalue of $$\mathbf{L}$$ and name the eigenvector `z_eig` in the next cell.


```python
# construct the Laplacian matrix
L = np.linalg.inv(D) @ (D - A)

# calculate the eigenvalues and eigenvectors of L
Lam, U = np.linalg.eig(L)

# get an array of indices that would sort the array of eigenvalues
ix = Lam.argsort()

# sort the eigenvalues and eigenvectors
Lam, U = Lam[ix], U[:,ix]

# find the second-smallest eigenvalue and the corresponding eigenvector
z_eig_val, z_eig = Lam[1], U[:,1]
```

Let's plot the data again, using one color for data points such that `z_eig[i] < 0` and another color for data points such that `z_eig[i] >= 0`.


```python
# plot the data
plt.scatter(X[:,0], X[:,1], c = z_eig >= 0)
```


    
![part-f.png](/images/part-f.png)
    


It looks like we're very close to correctly separate the data into two crescents, with only one data point mis-clustered.

## Part G

We're finally ready to write a function called `spectral_clustering(X, epsilon)` to perform spectral clustering.


```python
def spectral_clustering(X, epsilon):
    """
    Perform spectral clustering to separate the data 
    into two meaningful clusters.
    
    Parameters
    ----------
    X: numpy.ndarray(), an array of data 
    where we need spectral clustering 
    to make out two clusters in the data
    epsilon: float, the distance threshold 
    used to construct the similarity matrix
    
    Return
    ----------
    numpy.ndarray(), an array of binary labels 
    indicating whether data point i is in cluster C0 or cluster C1
    """
    
    # construct the similarity matrix
    A = 1 * (pairwise_distances(X) < epsilon)
    np.fill_diagonal(A, 0)

    # construct the Laplacian matrix
    D = np.diag(A.sum(axis = 1))
    L = np.linalg.inv(D) @ (D - A)
    
    # compute the eigenvector with the second-smallest eigenvalue 
    # of the Laplacian matrix
    Lam, U = np.linalg.eig(L)
    ix = Lam.argsort()
    z_eig_val, z_eig = Lam[ix][1], U[:,ix][:,1]
    
    # return labels based on the sign of each entry in the eigenvector
    return(1 * (z_eig < 0))
```

Let's demonstrate our function using the data.


```python
spectral_clustering(X, epsilon) # demonstration
```




    array([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
           1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
           0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
           0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
           0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
           0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
           0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
           1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
           0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
           0, 0])



This looks pretty good! We have one part of the data with labels equal to `1` and another part of the data with labels equal to `0`. The data points with labels equal to `0` are in cluster $$C_0$$, while the data points with labels equal to `1` are in cluster $$C_1$$.

## Part H

Let's increase `n` to `1000`, generate different data sets with the `make_moons` function by increasing the `noise` argument, and plot the data.


```python
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```


    
![part-h-1.png](/images/part-h-1.png)
    


When `noise=0.05`, we're able to correctly cluster the two crescents with no points mis-clustered.


```python
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.08, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```


    
![part-h-2.png](/images/part-h-2.png)
    


When `noise=0.08`, we're very close to correctly cluster the two crescents with two data points mis-clustered.


```python
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```


    
![part-h-3.png](/images/part-h-3.png)
    


When `noise=0.1`, our function are still able to cluster the two crescents, but more data points are mis-clustered.

## Part I

Let's now try our `spectral_clustering()` function on another data set -- the bull's eye.


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```


    
![two-circles.png](/images/two-circles.png)
    


As we can see, there're two concentric circles. We'll experiment with different values of `epsilon` to see for which value of `epsilon` the `spectral_clustering()` function are able to correctly separate the two circles. Try values of `epsilon` between `0` and `1.0`.


```python
epsilon = 0.85
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```


    
![part-i-1.png](/images/part-i-1.png)
    


When `epsilon = 0.85`, we are not able to correctly cluster the two rings.


```python
epsilon = 0.6
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```


    
![part-i-2.png](/images/part-i-2.png)
    


When `epsilon = 0.6`, we are not able to correctly cluster the two rings.


```python
epsilon = 0.5
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```


    
![part-i-3.png](/images/part-i-3.png)
    


When `epsilon = 0.5`, we're able to correctly separate the data into two rings with no points mis-clustered.
